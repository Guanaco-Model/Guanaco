# Guanaco: A Multilingual Instruction-Following Language Model Based on LLaMA 7B

As artificial intelligence technology rapidly advances, instruction-following language models have made significant progress in both academia and industry. In this context, we introduce a new language model called "Guanaco" to support a broader range of research and applications. Here is a brief overview of the Guanaco model.

## Model Overview

Guanaco is an instruction-following language model trained on Meta's LLaMA 7B model. Building upon the original 52K data from the Alpaca model, we added an additional 49,482 entries, covering Simplified Chinese, Traditional Chinese (Taiwan), Traditional Chinese (Hong Kong), and various linguistic and grammatical tasks. By retraining and optimizing the model with this rich data, Guanaco demonstrates excellent performance and potential in a multilingual environment.

## Dataset and Model Weights

To promote openness and replicability in research, we have made the Guanaco dataset publicly available and plan to release the model weights in the future. By providing these resources, we hope to encourage more researchers to engage in related research and jointly advance the development of instruction-following language models.

## Precautions

When using the Guanaco model, please note the following points:

The Guanaco model has not yet been filtered for harmful, biased, or explicit content. During use, outputs that do not conform to ethical norms may be generated. Please pay special attention to this issue in research or practical applications.

## Scope of Application

The Guanaco model aims to promote academic research on instruction-following language models and help researchers gain deeper insights into and improve the performance of these models. Please note that the Guanaco model is intended for academic research purposes only and is prohibited for any commercial use.

